{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianOuou/NLP-Course/blob/main/course/en/chapter2/section2_Behind_the_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4Jo7OKCEh4a"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvnSIyqrEh4c"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_eW0RF8hEh4d",
        "outputId": "4157eb72-60d1-4027-d4ae-36667954f7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "使用分词器进行预处理\n",
        "\n",
        "与其他神经网络一样，Transformer模型无法直接处理原始文本，因此流程的第一步是将文本输入转换为模型可理解的数字。为此我们使用分词器（tokenizer），其核心功能包括：\n",
        "\n",
        "分词：将输入拆分为单词、子词或符号（如标点）等基本单元（称为token）\n",
        "数值映射：将每个token转换为对应的整数\n",
        "附加输入：添加模型可能需要的其他辅助信息（如注意力掩码）"
      ],
      "metadata": {
        "id": "k42IpyW5F90P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "JHGOSUH0Eh4e",
        "outputId": "9543885a-f089-4052-a852-7d02cbc0df10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598049521446228},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "1WuEtJJlEh4f"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "4xAdzVWPEh4f",
        "outputId": "9968f63c-9df5-474b-b5a0-0106f5566998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
          ]
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "加载与使用模型\n",
        "\n",
        "我们可以像下载分词器一样下载预训练模型。🤗 Transformers 提供了 AutoModel 类，同样包含 from_pretrained() 方法：\n",
        "\n",
        "好的！我用一个更直观的例子来解释Transformer输出的三维向量结构，特别是\"隐藏大小（Hidden Size）\"这个关键维度。\n",
        "\n",
        "---\n",
        "\n",
        "### 以具体例子说明三维输出结构\n",
        "假设我们有以下两个句子组成一个batch：\n",
        "1. \"I love NLP!\"  \n",
        "2. \"Transformers are powerful.\"\n",
        "\n",
        "经过分词和填充后，每个句子被转换为长度为6的token IDs（假设填充后长度统一为6）。使用一个隐藏层大小为4的微型Transformer模型（实际模型隐藏层大得多，这里简化说明）：\n",
        "\n",
        "#### 1. 输入张量形状（模型接收的`input_ids`）\n",
        "```python\n",
        "shape = [batch_size, sequence_length] = [2, 6]\n",
        "```\n",
        "实际数值可能是：\n",
        "```\n",
        "[\n",
        "  [101, 1045, 2293, 17953, 999, 102],  # \"I love NLP!\" 的token IDs\n",
        "  [101, 19081, 2024, 3427, 1012, 102]  # \"Transformers are powerful.\" 的token IDs\n",
        "]\n",
        "```\n",
        "\n",
        "#### 2. 模型输出的隐藏状态（假设隐藏大小=4）\n",
        "```python\n",
        "shape = [batch_size, sequence_length, hidden_size] = [2, 6, 4]\n",
        "```\n",
        "实际输出可能类似：\n",
        "```python\n",
        "[\n",
        "  # 第一个句子的6个token，每个token用4维向量表示\n",
        "  [[0.1, 0.3, -0.2, 0.8],  # \"[CLS]\" token的表示\n",
        "   [0.5, 0.2, 0.6, -0.1],  # \"I\"\n",
        "   [0.3, 0.9, 0.4, 0.7],    # \"love\"\n",
        "   [0.8, 0.5, -0.3, 0.2],   # \"NLP\"\n",
        "   [0.2, 0.1, 0.0, 0.4],    # \"!\"\n",
        "   [0.6, 0.3, 0.1, -0.2]],  # \"[SEP]\"\n",
        "  \n",
        "  # 第二个句子的6个token\n",
        "  [[0.1, 0.3, -0.2, 0.8],   # \"[CLS]\"\n",
        "   [0.7, 0.4, 0.9, -0.5],   # \"Transformers\"\n",
        "   [0.2, 0.6, 0.3, 0.1],    # \"are\"\n",
        "   [0.4, 0.8, -0.2, 0.5],   # \"powerful\"\n",
        "   [0.3, 0.1, 0.7, 0.0],    # \".\"\n",
        "   [0.6, 0.3, 0.1, -0.2]]   # \"[SEP]\"\n",
        "]\n",
        "```\n",
        "\n",
        "#### 3. 为什么说\"高维\"？\n",
        "- **隐藏大小=4**（本例简化值）：\n",
        "  - 每个token被映射到4维空间的一个点（如\"love\" → [0.3, 0.9, 0.4, 0.7]）\n",
        "  - 类似用4个特征描述一个token的语义\n",
        "  \n",
        "- **实际模型（如BERT-base）的隐藏大小=768**：\n",
        "  - 每个token用768维向量表示\n",
        "  - 相当于用768个数值特征编码一个token的上下文信息\n",
        "  - 例如：\"bank\"在\"river bank\"和\"bank account\"中会得到不同的768维向量\n",
        "\n",
        "- **大模型（如GPT-3）的隐藏大小可达12288**：\n",
        "  - 每个token的表示空间维度极高\n",
        "  - 能捕获更细粒度的语义和语法特征\n",
        "\n",
        "#### 4. 三维结构的实际意义\n",
        "- **批处理维度**：同时处理多个句子（本例2个）\n",
        "- **序列维度**：保留每个token的位置信息（本例每个句子6个token）\n",
        "- **隐藏维度**：每个token的\"知识存储空间\"，维度越高表征能力越强\n",
        "\n",
        "---\n",
        "\n",
        "### 类比帮助理解\n",
        "把Transformer输出想象成一个立方体：\n",
        "```\n",
        "       隐藏大小（768）\n",
        "       ↑\n",
        "      / \\\n",
        "     /   \\\n",
        "    ┌─────┐\n",
        "    │     │ ← 一个token的表示（768个数值）\n",
        "    └─────┘\n",
        "   ↗\n",
        "序列长度（16）\n",
        "批大小（2） → 整个立方体包含 2×16×768 个数值\n",
        "```\n",
        "\n",
        "这种高维表示使得模型能区分：\n",
        "- 同形异义词：\"苹果\"（公司 vs 水果）\n",
        "- 复杂语义：\"虽然下雨了，但我很开心\"中的情感矛盾"
      ],
      "metadata": {
        "id": "c59bjS5jITSO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer模型通过高维隐藏表示（如768维或更高）能够区分复杂语义，这主要依赖于以下几个关键机制：\n",
        "\n",
        "### 1. **上下文感知的动态编码**\n",
        "   - **传统词向量问题**：像Word2Vec这样的静态嵌入会给\"苹果\"分配固定向量，无法区分\"苹果手机\"和\"吃苹果\"的不同含义。\n",
        "   - **Transformer的解决方案**：\n",
        "     - 通过自注意力机制，模型会根据句子上下文动态调整每个token的表示\n",
        "     - 示例：\n",
        "       ```python\n",
        "       # \"苹果\"在不同语境下的向量差异\n",
        "       苹果_公司 = [0.8, -0.2, 0.3, ..., 0.6]  # 与\"手机\"\"发布会\"等词关联\n",
        "       苹果_水果 = [0.3, 0.5, -0.7, ..., 0.1]  # 与\"吃\"\"甜\"\"水果\"等词关联\n",
        "       ```\n",
        "     - 余弦相似度计算可能显示这两个向量的相似度低于0.3（完全相同的向量为1.0）\n",
        "\n",
        "### 2. **注意力机制的多层次理解**\n",
        "   - **第一层注意力**（局部语法）：\n",
        "     - \"下雨\" → \"虽然\"（转折关系）\n",
        "     - \"开心\" → \"但\"（情感转折）\n",
        "   - **深层注意力**（语义组合）：\n",
        "     ```python\n",
        "     # 情感分析中的矛盾语义处理\n",
        "     虽然_vec = [0.2, -0.1, ..., 0.9]  # 携带转折预期\n",
        "     下雨_vec = [0.7, -0.8, ..., -0.5] # 负面情感倾向\n",
        "     但_vec   = [-0.3, 0.6, ..., 0.4]  # 强转折信号\n",
        "     开心_vec = [-0.9, 0.7, ..., 0.8]  # 正面情感\n",
        "     \n",
        "     # 模型通过注意力权重组合：\n",
        "     最终表示 = 0.3*虽然_vec + 0.1*下雨_vec + 0.4*但_vec + 0.9*开心_vec\n",
        "     ```\n",
        "\n",
        "### 3. **高维空间的几何特性**\n",
        "   - **表征能力**：\n",
        "     - 768维空间可以构造10^300+个不同的超平面（决策边界）\n",
        "     - 相比之下，50维Word2Vec只能构造约10^15个超平面\n",
        "   - **语义拓扑结构**：\n",
        "     ```\n",
        "     高维空间中：\n",
        "     \"苹果公司\" ——靠近——> \"科技\"/\"手机\"\n",
        "                   ↑\n",
        "                  (正交轴)\n",
        "                   ↓\n",
        "     \"苹果水果\" ——靠近——> \"食物\"/\"健康\"\n",
        "     ```\n",
        "\n",
        "### 4. **层级特征提取**\n",
        "   - **底层（靠近输入层）**：\n",
        "     - 识别词性/基本语法（\"下雨\"是动词，\"开心\"是形容词）\n",
        "   - **中层**：\n",
        "     - 捕捉短语级语义（\"下雨了\"→负面，\"很开心\"→正面）\n",
        "   - **高层（靠近输出层）**：\n",
        "     - 构建句子级理解（转折关系的整体情感倾向）\n",
        "\n",
        "### 5. **具体案例分析\n",
        "#### 案例1：同形异义词区分\n",
        "   ```python\n",
        "   # 输入句子1：\"苹果发布了新手机\"\n",
        "   [CLS] 苹果 发布 了 新 手机 [SEP]\n",
        "   \n",
        "   # 输入句子2：\"我买了一个苹果\"\n",
        "   [CLS] 我 买 了 一 个 苹果 [SEP]\n",
        "   \n",
        "   # 模型处理：\n",
        "   1. \"苹果\"的初始嵌入相同（相同的token ID）\n",
        "   2. 经过多层Transformer后：\n",
        "      - 句子1中的\"苹果\"受到\"发布\"\"手机\"等词的影响→向量偏向科技公司\n",
        "      - 句子2中的\"苹果\"受到\"买\"\"个\"等词的影响→向量偏向水果\n",
        "   3. 最终两者的余弦相似度可能＜0.4\n",
        "   ```\n",
        "\n",
        "#### 案例2：情感矛盾解析\n",
        "   ```python\n",
        "   # 输入句子：\"虽然下雨了，但我很开心\"\n",
        "   [CLS] 虽然 下雨 了 ， 但 我 很 开心 [SEP]\n",
        "   \n",
        "   # 关键步骤：\n",
        "   1. 注意力头1发现：\"虽然\"↔\"但\"（转折关系，权重0.8）\n",
        "   2. 注意力头2发现：\"下雨\"↔\"不开心\"（隐含关联，权重0.6）\n",
        "   3. 注意力头3发现：\"很开心\"↔\"但\"（情感强化，权重0.9）\n",
        "   4. 最终[CLS]位置汇集所有信息：\n",
        "      - 综合得分：正面情感0.7，负面情感0.3\n",
        "      - 分类结果：正面（尽管含有负面词汇）\n",
        "   ```\n",
        "\n",
        "### 为什么低维表示无法做到？\n",
        "假设只用20维向量：\n",
        "- 无法同时编码语法/情感/指代等多层次信息\n",
        "- 语义空间过于拥挤，不同含义的\"苹果\"向量会重叠\n",
        "- 难以建立复杂的转折关系（需要更高维度的正交基）\n",
        "\n",
        "而768+维空间可以：\n",
        "- 为每个语义维度分配独立的\"子空间\"\n",
        "- 通过线性变换构建数百万个潜在的特征组合\n",
        "- 保持不同含义向量的近似正交性（互不干扰）\n",
        "\n",
        "这种高维动态编码，正是Transformer理解复杂语言现象的核心优势。"
      ],
      "metadata": {
        "id": "2OZiOR0TI8lw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9SwigsOtEh4g"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Bqwwknj4Eh4g",
        "outputId": "a4211054-78bd-436a-df4b-66dc087bca13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 16, 768])\n"
          ]
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "J2QZoIOwEh4g"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "wr21DQYHEh4h",
        "outputId": "2795d533-b423-4823-946a-fac79ea1c246",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 2])\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "aEzrYIHxEh4h",
        "outputId": "48e5b009-67e8-4d39-b58a-aff863cbd777",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.5607,  1.6123],\n",
            "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 模型输出的后处理步骤解析\n",
        "\n",
        "当模型直接输出原始结果时，我们需要通过后处理使其具有可解释性。以下是关键步骤的详细说明：\n",
        "\n",
        "#### 1. 理解原始输出（Logits）\n",
        "模型最后一层输出的原始分数称为**logits**：\n",
        "```python\n",
        "tensor([[-1.5607,  1.6123],  # 第一个句子的logits\n",
        "        [ 4.1692, -3.3464]]) # 第二个句子的logits\n",
        "```\n",
        "- 这些数值没有概率意义\n",
        "- 正/负值仅表示相对置信度（如第一个句子中1.6123 > -1.5607，倾向POSITIVE）\n",
        "\n",
        "#### 2. 转换为概率（SoftMax处理）\n",
        "通过SoftMax函数将logits转换为概率分布：\n",
        "```python\n",
        "import torch\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "```\n",
        "输出结果：\n",
        "```python\n",
        "tensor([[0.0402, 0.9598],  # 第一个句子的概率\n",
        "        [0.9995, 0.0005]]) # 第二个句子的概率\n",
        "```\n",
        "- `dim=-1` 表示对最后一个维度（标签维度）做归一化\n",
        "- 每行两个值的和为1（符合概率分布特性）\n",
        "\n",
        "#### 3. 标签映射\n",
        "通过模型配置查看标签对应关系：\n",
        "```python\n",
        "model.config.id2label  # 输出: {0: 'NEGATIVE', 1: 'POSITIVE'}\n",
        "```\n",
        "最终预测结果：\n",
        "- **第一个句子**：\"I've been waiting...\"  \n",
        "  → POSITIVE (95.98%置信度)  \n",
        "- **第二个句子**：\"I hate this...\"  \n",
        "  → NEGATIVE (99.95%置信度)\n",
        "\n",
        "#### 技术细节说明\n",
        "| 步骤 | 输入 | 操作 | 输出 | 目的 |\n",
        "|------|------|------|------|------|\n",
        "| 模型原始输出 | 文本特征 | 线性层 | Logits | 获得原始分数 |\n",
        "| SoftMax | Logits | e^x/sum(e^x) | 概率 | 数值归一化 |\n",
        "| 标签映射 | 概率 | id2label | 标签 | 人类可读结果 |\n",
        "\n",
        "#### 为什么使用Logits而非直接输出概率？\n",
        "1. **训练效率**：  \n",
        "   交叉熵损失函数会合并SoftMax计算，减少数值计算步骤\n",
        "2. **数值稳定性**：  \n",
        "   在反向传播时直接处理logits可避免梯度消失问题\n",
        "3. **灵活性**：  \n",
        "   方便后续接不同的损失函数（如带权重的交叉熵）\n",
        "\n",
        "#### 可视化理解\n",
        "```\n",
        "原始文本 → [Tokenization] → 模型处理 → Logits → SoftMax → 概率 → 标签映射\n",
        "           (预处理)        (前向传播)  ([-1.56,1.61]) → [0.04,0.96] → \"POSITIVE\"\n",
        "```\n",
        "\n",
        "通过这三个步骤，我们完整复现了pipeline的工作流程。这种模块化设计既保证了灵活性（可单独调整任一步骤），又确保了结果的可解释性。"
      ],
      "metadata": {
        "id": "o8LvOOn9KJM5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "yRA308TwEh4i",
        "outputId": "72ca1f85-e9b3-4176-8f4a-bc3b26ed32a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[4.0195e-02, 9.5980e-01],\n",
            "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "DKHqSbGxEh4i",
        "outputId": "c7bbd6e4-67e6-4ff3-a775-39c259679231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Behind the pipeline (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}