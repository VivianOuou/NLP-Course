{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianOuou/NLP-Course/blob/main/course/en/chapter2/section3_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U97mLME3NCrW"
      },
      "source": [
        "# Models (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXNrXZH0NCra"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DZKcPO0-NCrb",
        "outputId": "bcfdfa79-b609-41d8-878c-4f8fee58f8d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer 模型的不同加载方式详解\n",
        "\n",
        "#### 1. 随机初始化模型（从零开始训练）\n",
        "```python\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# 创建默认配置\n",
        "config = BertConfig()\n",
        "\n",
        "# 初始化随机权重的模型\n",
        "model = BertModel(config)\n",
        "```\n",
        "**特点**：\n",
        "- 所有权重随机生成\n",
        "- 输出无意义的噪声数据\n",
        "- 需要从头训练（需大量数据和计算资源）\n",
        "- 适用场景：\n",
        "  - 学术研究新型架构\n",
        "  - 特定领域全新预训练\n",
        "\n",
        "> 📌 **注意**：完整的BERT预训练通常需要：\n",
        "> - 16+块TPU/GPU\n",
        "> - 数十GB文本数据\n",
        "> - 数天到数周训练时间\n",
        "\n",
        "#### 2. 加载预训练模型（推荐方式）\n",
        "```python\n",
        "from transformers import BertModel\n",
        "\n",
        "# 加载Google官方预训练模型\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "```\n",
        "**优势**：\n",
        "- 立即获得语言理解能力\n",
        "- 支持快速微调（Fine-tuning）\n",
        "- 节省大量计算资源\n",
        "\n",
        "**模型标识符说明**：\n",
        "- `bert-base-cased`：区分大小写的Base版\n",
        "- `bert-large-uncased`：不区分大小写的Large版\n",
        "- 更多变体可在[Hugging Face Model Hub](https://huggingface.co/models)查询\n",
        "\n",
        "#### 3. 使用AutoModel的通用加载（最佳实践）\n",
        "```python\n",
        "from transformers import AutoModel\n",
        "\n",
        "# 兼容所有BERT类模型的加载方式\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# 同样适用于其他架构\n",
        "roberta = AutoModel.from_pretrained(\"roberta-base\")\n",
        "distilbert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "```\n",
        "**为什么推荐？**\n",
        "- 代码可移植性强\n",
        "- 无需修改代码即可切换模型架构\n",
        "- 自动识别模型类型\n",
        "\n",
        "#### 4. 模型缓存机制\n",
        "首次加载时的自动行为：\n",
        "1. 下载模型权重到缓存目录（默认`~/.cache/huggingface/transformers`）\n",
        "2. 后续调用直接读取本地缓存\n",
        "\n",
        "**自定义缓存位置**：\n",
        "```bash\n",
        "# 设置环境变量（Linux/macOS）\n",
        "export HF_HOME=/path/to/your/cache\n",
        "\n",
        "# Windows\n",
        "set HF_HOME=C:\\path\\to\\your\\cache\n",
        "```\n",
        "\n",
        "#### 5. 模型加载过程图解\n",
        "```mermaid\n",
        "sequenceDiagram\n",
        "    User->>HuggingFace: from_pretrained(\"bert-base-cased\")\n",
        "    HuggingFace->>Cache: 检查本地缓存\n",
        "    alt 缓存存在\n",
        "        Cache->>User: 直接加载\n",
        "    else 缓存不存在\n",
        "        HuggingFace->>Model Hub: 下载模型\n",
        "        Model Hub->>HuggingFace: 返回权重文件\n",
        "        HuggingFace->>Cache: 存储到本地\n",
        "        Cache->>User: 加载模型\n",
        "    end\n",
        "```\n",
        "\n",
        "#### 6. 不同加载方式对比\n",
        "| 特性                | 随机初始化               | 预训练加载               | AutoModel加载         |\n",
        "|--------------------|------------------------|------------------------|----------------------|\n",
        "| **初始化方式**      | 随机权重                | 预训练权重              | 预训练权重           |\n",
        "| **代码兼容性**      | 仅限指定架构            | 仅限指定架构            | 支持所有架构         |\n",
        "| **训练需求**        | 需完整训练              | 可选择性微调            | 可选择性微调         |\n",
        "| **典型应用**        | 新型模型研发            | 特定架构优化            | 快速实验/生产部署    |\n",
        "\n"
      ],
      "metadata": {
        "id": "u0jhY0m4TGcz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qn7kjDhcNCrd"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Building the config\n",
        "config = BertConfig()\n",
        "\n",
        "# Building the model from the config\n",
        "model = BertModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7O0qoCO8NCre",
        "outputId": "bf2a2b3d-3a19-4d00-e0ed-21e21f1c3bbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "miNnq4jzNCrf"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertModel(config)\n",
        "\n",
        "\n",
        "# Model is randomly initialized!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JSKaFdVCNCrf"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer 模型保存方法详解\n",
        "\n",
        "#### 1. 基础保存方法\n",
        "使用`save_pretrained()`方法可将模型保存到指定目录：\n",
        "```python\n",
        "model.save_pretrained(\"./my_bert_model/\")\n",
        "```\n",
        "这将生成两个核心文件：\n",
        "```\n",
        "my_bert_model/\n",
        "├── config.json        # 模型架构配置\n",
        "└── pytorch_model.bin  # 模型权重参数\n",
        "```\n",
        "\n",
        "#### 2. 文件功能解析\n",
        "| 文件名              | 内容类型 | 作用                                                                 |\n",
        "|---------------------|----------|----------------------------------------------------------------------|\n",
        "| `config.json`       | JSON文本 | 包含模型架构参数（如`hidden_size=768`）和训练元数据（框架版本等）     |\n",
        "| `pytorch_model.bin` | 二进制   | 存储所有可训练参数的\"状态字典\"（state_dict）                         |\n",
        "\n",
        "#### 3. 技术细节说明\n",
        "- **PyTorch用户**：`pytorch_model.bin`是标准的PyTorch状态字典文件\n",
        "- **TensorFlow用户**：保存时会额外生成`tf_model.h5`文件\n",
        "- **多框架支持**：通过`save_pretrained(..., save_format=\"tf\")`指定格式\n",
        "\n",
        "#### 4. 完整保存示例\n",
        "```python\n",
        "from transformers import AutoModel\n",
        "\n",
        "# 加载预训练模型\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# 进行微调训练...\n",
        "# model.train(...)\n",
        "\n",
        "# 保存完整模型\n",
        "model.save_pretrained(\n",
        "    \"./fine_tuned_bert/\",\n",
        "    save_config=True,  # 默认True\n",
        "    save_function=torch.save,  # 可自定义保存方法\n",
        "    state_dict=model.state_dict()  # 可传入自定义state_dict\n",
        ")\n",
        "```\n",
        "\n",
        "#### 5. 文件内容示例\n",
        "**config.json**:\n",
        "```json\n",
        "{\n",
        "  \"architectures\": [\"BertForMaskedLM\"],\n",
        "  \"hidden_size\": 768,\n",
        "  \"num_attention_heads\": 12,\n",
        "  \"model_type\": \"bert\",\n",
        "  \"transformers_version\": \"4.28.1\"\n",
        "}\n",
        "```\n",
        "\n",
        "**pytorch_model.bin**结构：\n",
        "```python\n",
        "{\n",
        "  'embeddings.word_embeddings.weight': tensor(...),\n",
        "  'encoder.layer.0.attention.self.query.weight': tensor(...),\n",
        "  # ...其他数千个参数...\n",
        "}\n",
        "```\n",
        "\n",
        "#### 6. 模型复用方法\n",
        "保存后可通过两种方式重新加载：\n",
        "```python\n",
        "# 方式1：直接加载\n",
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"./fine_tuned_bert/\")\n",
        "\n",
        "# 方式2：先加载配置再构建模型\n",
        "from transformers import BertConfig, BertModel\n",
        "config = BertConfig.from_pretrained(\"./fine_tuned_bert/config.json\")\n",
        "model = BertModel.from_pretrained(\"./fine_tuned_bert/\", config=config)\n",
        "```\n",
        "\n",
        "#### 7. 高级保存选项\n",
        "- **仅保存权重**：\n",
        "  ```python\n",
        "  torch.save(model.state_dict(), \"weights_only.pt\")\n",
        "  ```\n",
        "- **保存为不同格式：\n",
        "  ```python\n",
        "  model.save_pretrained(\"./tf_model/\", save_format=\"tf\")  # 保存TensorFlow格式\n",
        "  ```\n",
        "\n",
        "#### 8. 生产环境最佳实践\n",
        "1. **版本控制**：将模型文件纳入Git LFS管理\n",
        "2. **压缩分发**：使用`tar -czvf model.tar.gz ./model/`打包\n",
        "3. **安全考虑**：\n",
        "   ```python\n",
        "   # 保存时排除敏感信息\n",
        "   model.config.to_dict().pop(\"api_key\", None)\n",
        "   ```\n",
        "\n",
        "#### 9. 常见问题解答\n",
        "**Q：为什么需要同时保存config和weights？**\n",
        "A：就像建筑需要\"设计图+建筑材料\"，config定义网络结构，weights提供具体参数值\n",
        "\n",
        "**Q：如何保存到云存储？**\n",
        "```python\n",
        "# 示例：保存到Google Drive\n",
        "model.save_pretrained(\"/content/drive/MyDrive/models/\")\n",
        "```\n",
        "\n",
        "通过这种保存机制，您可以：\n",
        "- 完整复现模型训练状态\n",
        "- 方便地共享模型\n",
        "- 灵活切换训练/推理环境\n",
        "\n",
        "> 💡 提示：使用`push_to_hub()`方法可直接将模型上传到Hugging Face Hub：\n",
        "> ```python\n",
        "> model.push_to_hub(\"my-awesome-model\")\n",
        "> ```"
      ],
      "metadata": {
        "id": "PFJsCyKaTcHP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xvAeCatBNCrf"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a Transformer model for inference\n",
        "\n",
        "Now that you know how to load and save a model, let’s try using it to make some predictions. Transformer models can only process numbers — numbers that the tokenizer generates. But before we discuss tokenizers, let’s explore what inputs the model accepts.\n",
        "\n",
        "Tokenizers can take care of casting the inputs to the appropriate framework’s tensors, but to help you understand what’s going on, we’ll take a quick look at what must be done before sending the inputs to the model."
      ],
      "metadata": {
        "id": "R5lsU_OZT9RX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "agj4vCDVNCrg"
      },
      "outputs": [],
      "source": [
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VfRe0f0KNCrg"
      },
      "outputs": [],
      "source": [
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V1FoHHBoNCrg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model_inputs = torch.tensor(encoded_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FLClRdT9NCrh"
      },
      "outputs": [],
      "source": [
        "output = model(model_inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Models (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}