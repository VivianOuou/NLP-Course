{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianOuou/NLP-Course/blob/main/course/en/chapter2/section5_Handling_multiple_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek87uWQ01oeb"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxvzFfx1oed"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rRjq0W1y1oee",
        "outputId": "4a2e3f37-c5de-4fa3-dcf0-35636dcd5b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "成功和不成功的区别主要在于 **输入张量的维度** 是否符合模型的预期。以下是关键对比：\n",
        "\n",
        "---\n",
        "\n",
        "### ❌ 失败的情况（报错 `IndexError`）\n",
        "```python\n",
        "input_ids = torch.tensor(ids)  # 形状为 (n,)，例如 (14,)\n",
        "model(input_ids)\n",
        "```\n",
        "- **问题**：  \n",
        "  输入的 `input_ids` 是一个 **一维张量**（形状为 `(序列长度,)`），但模型默认需要 **二维张量**（形状为 `(batch_size, 序列长度)`），即必须包含批次维度。\n",
        "- **错误原因**：  \n",
        "  模型试图访问 `batch_size` 维度（第0维），但输入的张量只有1维（序列长度），导致维度越界。\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ 成功的情况\n",
        "```python\n",
        "input_ids = torch.tensor([ids])  # 形状为 (1, n)，例如 (1, 14)\n",
        "model(input_ids)\n",
        "```\n",
        "- **关键区别**：  \n",
        "  通过 `[ids]` 将输入包装为一个列表，`torch.tensor()` 会自动将其转换为 **二维张量**，显式添加了批次维度（`batch_size=1`）。\n",
        "- **符合模型预期**：  \n",
        "  模型需要 `(batch_size, 序列长度)` 的输入，这里 `batch_size=1` 表示单样本批次。\n",
        "\n",
        "---\n",
        "\n",
        "### 🌰 直观类比\n",
        "假设原始 `ids` 是 `[1, 2, 3]`：\n",
        "- ❌ 错误输入：`torch.tensor([1, 2, 3])` → 形状 `(3,)`（模型无法处理）。\n",
        "- ✅ 正确输入：`torch.tensor([[1, 2, 3]])` → 形状 `(1, 3)`（模型可处理）。\n",
        "\n",
        "---\n",
        "\n",
        "### 为什么模型需要批次输入？\n",
        "1. **效率**：GPU 擅长并行计算，批量处理比逐条处理更快。\n",
        "2. **统一接口**：无论输入单条还是多条数据，模型始终以批次形式处理（单条时 `batch_size=1`）。\n",
        "\n",
        "---\n",
        "\n",
        "### 补充：批处理多序列\n",
        "如果输入多个序列，需保证长度一致（通过填充）：\n",
        "```python\n",
        "batched_ids = [\n",
        "    [1, 2, 3],\n",
        "    [1, 2, 0]  # 假设用0填充到长度3\n",
        "]\n",
        "input_ids = torch.tensor(batched_ids)  # 形状 (2, 3)\n",
        "model(input_ids)\n",
        "```\n",
        "\n",
        "总结：**始终确保输入张量有 `batch_size` 维度**，这是成功运行的关键！"
      ],
      "metadata": {
        "id": "4O9q-mKk3D3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dh5Z3hyX1oef",
        "outputId": "57e2e6a2-c916-419e-eed3-a603e40a42b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2b9fc1b6cd40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# This line will fail.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5166\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5167\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5168\u001b[0m             warn_string = (\n\u001b[1;32m   5169\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0YDI5uA61oeg",
        "outputId": "fc651245-46c1-4b1c-9534-ee243c6f5d94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n"
          ]
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1aG75ZxX1oeg",
        "outputId": "0a17c5ac-d954-41c4-fbb9-f810e2394bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "#把ids转化为tensor，再输入进行模型中\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 填充输入（Padding the Inputs）\n",
        "\n",
        "以下列表的列表无法直接转换为张量：\n",
        "\n",
        "```python\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]\n",
        "```\n",
        "\n",
        "为了解决这个问题，我们可以使用 **填充（padding）** 来使张量成为矩形形状。填充通过向较短的句子添加一个特殊的 **填充标记（padding token）**，确保所有句子的长度相同。例如，如果你有 10 个句子，每个句子有 10 个单词，而 1 个句子有 20 个单词，填充会确保所有句子都变成 20 个单词。在我们的例子中，填充后的张量如下：\n",
        "\n",
        "```python\n",
        "padding_id = 100  # 假设填充标记的ID是100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],          # 第一个句子（长度3）\n",
        "    [200, 200, padding_id],   # 第二个句子（填充后长度3）\n",
        "]\n",
        "```\n",
        "\n",
        "填充标记的 ID 可以通过 `tokenizer.pad_token_id` 获取。让我们使用它，并分别将两个句子单独输入模型，以及组成批次后输入模型：\n",
        "\n",
        "```python\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]  # 句子1（长度3）\n",
        "sequence2_ids = [[200, 200]]       # 句子2（长度2）\n",
        "batched_ids = [\n",
        "    [200, 200, 200],               # 句子1\n",
        "    [200, 200, tokenizer.pad_token_id],  # 句子2（填充后长度3）\n",
        "]\n",
        "\n",
        "# 分别输入模型\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "\n",
        "# 批量输入模型\n",
        "print(model(torch.tensor(batched_ids)).logits)\n",
        "```\n",
        "\n",
        "输出结果：\n",
        "\n",
        "```\n",
        "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)       # 句子1单独输入\n",
        "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)       # 句子2单独输入\n",
        "tensor([[ 1.5694, -1.3895],                                 # 批量输入\n",
        "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n",
        "```\n",
        "\n",
        "### 问题：批量预测的 logits 不正确\n",
        "在批量预测的结果中，第二行的 logits 应该和单独输入句子2时的 logits（`[0.5803, -0.4125]`）相同，但我们却得到了完全不同的值（`[1.3373, -1.2163]`）！\n",
        "\n",
        "### 原因：注意力机制会考虑填充标记\n",
        "Transformer 模型的核心特点是 **注意力层（attention layers）**，它们会对序列中的每个 token 进行上下文建模。这意味着，填充标记（如 `padding_id=100`）也会被纳入计算，因为注意力机制会关注序列中的所有 token（包括填充 token）。\n",
        "\n",
        "### 解决方案：使用注意力掩码（Attention Mask）\n",
        "为了确保：\n",
        "1. **单独输入不同长度的句子** 和  \n",
        "2. **批量输入填充后的句子**  \n",
        "\n",
        "得到相同的结果，我们需要告诉注意力层 **忽略填充标记**。这可以通过 **注意力掩码（attention mask）** 实现。\n",
        "\n",
        "#### 注意力掩码的作用\n",
        "- **1**：表示模型应关注该 token（真实 token）。  \n",
        "- **0**：表示模型应忽略该 token（填充 token）。  \n",
        "\n",
        "#### 修改后的代码（使用 `tokenizer` 自动生成掩码）\n",
        "```python\n",
        "# 使用 tokenizer 自动处理填充和掩码\n",
        "batched_inputs = tokenizer(\n",
        "    [\"Sentence one\", \"Sentence two\"],  # 两个句子\n",
        "    padding=True,                      # 自动填充\n",
        "    return_tensors=\"pt\",               # 返回 PyTorch 张量\n",
        ")\n",
        "\n",
        "# 输入模型（包含 input_ids 和 attention_mask）\n",
        "outputs = model(**batched_inputs)\n",
        "print(outputs.logits)\n",
        "```\n",
        "\n",
        "这样，模型会正确忽略填充部分，批量计算的结果将与单独计算的结果一致。\n",
        "\n",
        "---\n",
        "\n",
        "### 关键总结\n",
        "1. **填充（Padding）**：使所有句子长度相同，以便组成张量。  \n",
        "2. **注意力掩码（Attention Mask）**：确保模型忽略填充 token，避免影响预测结果。  \n",
        "3. **最佳实践**：直接使用 `tokenizer(..., padding=True, return_tensors=\"pt\")`，让分词器自动处理填充和掩码，而非手动操作。  \n",
        "\n",
        "> 📌 **记住**：**填充 + 掩码** 是正确处理变长序列批处理的关键！"
      ],
      "metadata": {
        "id": "tiAkAWOY4V5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ldP7HMUq1oeh"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fDzeNdkz1oeh"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3UNff1_T1oei",
        "outputId": "a0c95c5c-da4e-40b0-abfd-1abfb3dd9248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NncIW4s_1oei",
        "outputId": "7651c384-a4cc-4804-e15f-b26771a4109b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#练习1\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# 句子\n",
        "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "sequence2 = \"I hate this so much!\"\n",
        "\n",
        "# 单独分词并转换为ID\n",
        "tokens1 = tokenizer.tokenize(sequence1)\n",
        "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "\n",
        "tokens2 = tokenizer.tokenize(sequence2)\n",
        "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "\n",
        "print(\"句子1的ID:\", ids1)\n",
        "print(\"句子2的ID:\", ids2)"
      ],
      "metadata": {
        "id": "KBF3ixkg4p_m",
        "outputId": "90df970d-e43e-4d6f-8e5a-3826092ddcb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "句子1的ID: [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
            "句子2的ID: [1045, 5223, 2023, 2061, 2172, 999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 转换为张量（添加批次维度）\n",
        "input_ids1 = torch.tensor([ids1])\n",
        "input_ids2 = torch.tensor([ids2])\n",
        "\n",
        "# 单独预测\n",
        "output1 = model(input_ids1)\n",
        "output2 = model(input_ids2)\n",
        "\n",
        "print(\"句子1的logits:\", output1.logits)\n",
        "print(\"句子2的logits:\", output2.logits)"
      ],
      "metadata": {
        "id": "6rNc77vr4tHV",
        "outputId": "2b536b1a-7ed3-4349-c434-28f8d9a1b17a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "句子1的logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n",
            "句子2的logits: tensor([[ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算最大长度\n",
        "max_length = max(len(ids1), len(ids2))\n",
        "\n",
        "# 对句子2填充（句子1已经是最长，无需填充）\n",
        "padded_ids2 = ids2 + [tokenizer.pad_token_id] * (max_length - len(ids2))\n",
        "\n",
        "# 填充后的批次输入\n",
        "batched_ids = [\n",
        "    ids1,              # 句子1（长度14）\n",
        "    padded_ids2        # 句子2（填充后长度14）\n",
        "]\n",
        "\n",
        "# 创建注意力掩码（1=真实token，0=填充token）\n",
        "attention_mask = [\n",
        "    [1] * len(ids1) + [0] * (max_length - len(ids1)),  # 句子1（无需填充，全1）\n",
        "    [1] * len(ids2) + [0] * (max_length - len(ids2))   # 句子2（前5个是真实token）\n",
        "]\n",
        "\n",
        "# 转换为张量\n",
        "input_ids = torch.tensor(batched_ids)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "print(\"填充后的input_ids:\\n\", input_ids)\n",
        "print(\"注意力掩码:\\n\", attention_mask)"
      ],
      "metadata": {
        "id": "71GlRqpZ4v8D",
        "outputId": "9344fbbf-d810-44ff-d12d-ca1f2a72567b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "填充后的input_ids:\n",
            " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012],\n",
            "        [ 1045,  5223,  2023,  2061,  2172,   999,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "注意力掩码:\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 输入模型（同时传递input_ids和attention_mask）\n",
        "batched_output = model(input_ids, attention_mask=attention_mask)\n",
        "print(\"批量预测的logits:\\n\", batched_output.logits)"
      ],
      "metadata": {
        "id": "mgphqm6B5Lek",
        "outputId": "33cd9fa1-be25-4c79-fc29-319b3f7bbcac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "批量预测的logits:\n",
            " tensor([[-2.7276,  2.8789],\n",
            "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 错误示范：不带掩码的批量输入\n",
        "wrong_output = model(input_ids)  # 不传递attention_mask\n",
        "print(\"不带掩码的logits（错误）:\\n\", wrong_output.logits)"
      ],
      "metadata": {
        "id": "kzSxpv6S5Ms9",
        "outputId": "1c47f187-91dd-4b96-c13d-4e80e3b37703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "不带掩码的logits（错误）:\n",
            " tensor([[-2.7276,  2.8789],\n",
            "        [ 2.5423, -2.1265]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 处理长序列问题（Longer Sequences）\n",
        "\n",
        "Transformer 模型对输入序列的长度有限制（通常为 512 或 1024 个 token）。如果序列超出限制，模型会报错。以下是解决方案和示例：\n",
        "\n",
        "---\n",
        "\n",
        "#### ❌ 问题示例\n",
        "假设有一个超长序列：\n",
        "```python\n",
        "long_sequence = \"This is a very long sequence... [hundreds of words later] ... and it will crash the model.\"\n",
        "```\n",
        "\n",
        "#### ✅ 解决方案 1：截断序列（Truncation）\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# 自动截断到模型最大长度（如512）\n",
        "truncated_sequence = tokenizer(\n",
        "    long_sequence,\n",
        "    truncation=True,  # 启用截断\n",
        "    max_length=512,   # 明确指定最大长度（可选）\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(\"截断后的输入长度:\", len(truncated_sequence[\"input_ids\"][0]))\n",
        "```\n",
        "\n",
        "#### ✅ 解决方案 2：使用支持长序列的模型\n",
        "```python\n",
        "from transformers import LongformerModel\n",
        "\n",
        "# 加载长序列专用模型（如Longformer，支持4096 token）\n",
        "model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 关键细节\n",
        "1. **默认限制**：\n",
        "   - BERT类模型：通常 512 token\n",
        "   - GPT类模型：通常 1024 token\n",
        "\n",
        "2. **截断方式**：\n",
        "   ```python\n",
        "   # 保留开头部分（默认）\n",
        "   tokenizer(sequence, truncation=True, max_length=100)\n",
        "\n",
        "   # 保留结尾部分\n",
        "   tokenizer(sequence, truncation=\"only_last\", max_length=100)\n",
        "   ```\n",
        "\n",
        "3. **长序列模型示例**：\n",
        "   - **Longformer**：支持 4096 token（适合文档处理）\n",
        "   - **LED**：支持 16384 token（适合超长文本摘要）\n",
        "\n",
        "---\n",
        "\n",
        "### 实际应用建议\n",
        "```python\n",
        "# 最佳实践：同时处理填充和截断\n",
        "inputs = tokenizer(\n",
        "    [sequence1, sequence2],\n",
        "    padding=True,      # 自动填充\n",
        "    truncation=True,   # 自动截断\n",
        "    max_length=512,    # 设置最大值\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "> 📌 **注意**：如果任务必须使用完整长序列（如法律文档分析），优先选择专用模型而非截断。"
      ],
      "metadata": {
        "id": "DhNeK6zP5kXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xFUyCHkG1oej",
        "outputId": "5e3c8977-eee9-4363-ece6-3fe75044896d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'max_sequence_length' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-af6acd4b2a78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'max_sequence_length' is not defined"
          ]
        }
      ],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个错误是因为你直接使用了未定义的变量 `max_sequence_length`。Python会报`NameError`，表示它不知道`max_sequence_length`是什么。\n",
        "\n",
        "### 错误原因分析\n",
        "```python\n",
        "sequence = sequence[:max_sequence_length]  # 报错\n",
        "```\n",
        "- 你试图用切片操作截取序列，但`max_sequence_length`这个变量从未被定义过\n",
        "- Python不知道`max_sequence_length`应该取值多少\n",
        "\n",
        "### 正确做法\n",
        "\n",
        "#### 方法1：直接使用固定值\n",
        "```python\n",
        "max_sequence_length = 512  # 先定义这个变量\n",
        "sequence = sequence[:max_sequence_length]  # 现在可以正常工作\n",
        "```\n",
        "\n",
        "#### 方法2：使用tokenizer的自动截断（推荐）\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = tokenizer(sequence, truncation=True, max_length=512)  # 自动处理截断\n",
        "```\n",
        "\n",
        "#### 方法3：获取模型的最大长度\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_sequence_length = tokenizer.model_max_length  # 获取模型支持的最大长度\n",
        "sequence = sequence[:max_sequence_length]\n",
        "```\n",
        "\n",
        "### 关键区别\n",
        "| 方法 | 优点 | 缺点 |\n",
        "|------|------|------|\n",
        "| 固定值 | 简单直接 | 需要知道模型限制 |\n",
        "| tokenizer自动截断 | 智能处理，保留重要部分 | 需要调用tokenizer |\n",
        "| 获取模型最大长度 | 动态适应不同模型 | 仍需手动切片 |\n",
        "\n",
        "### 最佳实践建议\n",
        "```python\n",
        "# 推荐做法：使用tokenizer自动处理\n",
        "inputs = tokenizer(\n",
        "    sequence,\n",
        "    truncation=True,  # 自动截断\n",
        "    max_length=512,   # 明确限制长度\n",
        "    return_tensors=\"pt\"  # 返回PyTorch张量\n",
        ")\n",
        "```\n",
        "\n",
        "> 这样既避免了未定义变量的错误，又能确保正确处理序列长度限制。Tokenizer会自动处理截断位置，通常比简单的前512字符截取效果更好。"
      ],
      "metadata": {
        "id": "sOzAehYf58Lc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}