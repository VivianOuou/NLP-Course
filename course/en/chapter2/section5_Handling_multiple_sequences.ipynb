{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianOuou/NLP-Course/blob/main/course/en/chapter2/section5_Handling_multiple_sequences.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ek87uWQ01oeb"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TxvzFfx1oed"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rRjq0W1y1oee",
        "outputId": "4a2e3f37-c5de-4fa3-dcf0-35636dcd5b80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "æˆåŠŸå’Œä¸æˆåŠŸçš„åŒºåˆ«ä¸»è¦åœ¨äº **è¾“å…¥å¼ é‡çš„ç»´åº¦** æ˜¯å¦ç¬¦åˆæ¨¡å‹çš„é¢„æœŸã€‚ä»¥ä¸‹æ˜¯å…³é”®å¯¹æ¯”ï¼š\n",
        "\n",
        "---\n",
        "\n",
        "### âŒ å¤±è´¥çš„æƒ…å†µï¼ˆæŠ¥é”™ `IndexError`ï¼‰\n",
        "```python\n",
        "input_ids = torch.tensor(ids)  # å½¢çŠ¶ä¸º (n,)ï¼Œä¾‹å¦‚ (14,)\n",
        "model(input_ids)\n",
        "```\n",
        "- **é—®é¢˜**ï¼š  \n",
        "  è¾“å…¥çš„ `input_ids` æ˜¯ä¸€ä¸ª **ä¸€ç»´å¼ é‡**ï¼ˆå½¢çŠ¶ä¸º `(åºåˆ—é•¿åº¦,)`ï¼‰ï¼Œä½†æ¨¡å‹é»˜è®¤éœ€è¦ **äºŒç»´å¼ é‡**ï¼ˆå½¢çŠ¶ä¸º `(batch_size, åºåˆ—é•¿åº¦)`ï¼‰ï¼Œå³å¿…é¡»åŒ…å«æ‰¹æ¬¡ç»´åº¦ã€‚\n",
        "- **é”™è¯¯åŸå› **ï¼š  \n",
        "  æ¨¡å‹è¯•å›¾è®¿é—® `batch_size` ç»´åº¦ï¼ˆç¬¬0ç»´ï¼‰ï¼Œä½†è¾“å…¥çš„å¼ é‡åªæœ‰1ç»´ï¼ˆåºåˆ—é•¿åº¦ï¼‰ï¼Œå¯¼è‡´ç»´åº¦è¶Šç•Œã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### âœ… æˆåŠŸçš„æƒ…å†µ\n",
        "```python\n",
        "input_ids = torch.tensor([ids])  # å½¢çŠ¶ä¸º (1, n)ï¼Œä¾‹å¦‚ (1, 14)\n",
        "model(input_ids)\n",
        "```\n",
        "- **å…³é”®åŒºåˆ«**ï¼š  \n",
        "  é€šè¿‡ `[ids]` å°†è¾“å…¥åŒ…è£…ä¸ºä¸€ä¸ªåˆ—è¡¨ï¼Œ`torch.tensor()` ä¼šè‡ªåŠ¨å°†å…¶è½¬æ¢ä¸º **äºŒç»´å¼ é‡**ï¼Œæ˜¾å¼æ·»åŠ äº†æ‰¹æ¬¡ç»´åº¦ï¼ˆ`batch_size=1`ï¼‰ã€‚\n",
        "- **ç¬¦åˆæ¨¡å‹é¢„æœŸ**ï¼š  \n",
        "  æ¨¡å‹éœ€è¦ `(batch_size, åºåˆ—é•¿åº¦)` çš„è¾“å…¥ï¼Œè¿™é‡Œ `batch_size=1` è¡¨ç¤ºå•æ ·æœ¬æ‰¹æ¬¡ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸŒ° ç›´è§‚ç±»æ¯”\n",
        "å‡è®¾åŸå§‹ `ids` æ˜¯ `[1, 2, 3]`ï¼š\n",
        "- âŒ é”™è¯¯è¾“å…¥ï¼š`torch.tensor([1, 2, 3])` â†’ å½¢çŠ¶ `(3,)`ï¼ˆæ¨¡å‹æ— æ³•å¤„ç†ï¼‰ã€‚\n",
        "- âœ… æ­£ç¡®è¾“å…¥ï¼š`torch.tensor([[1, 2, 3]])` â†’ å½¢çŠ¶ `(1, 3)`ï¼ˆæ¨¡å‹å¯å¤„ç†ï¼‰ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### ä¸ºä»€ä¹ˆæ¨¡å‹éœ€è¦æ‰¹æ¬¡è¾“å…¥ï¼Ÿ\n",
        "1. **æ•ˆç‡**ï¼šGPU æ“…é•¿å¹¶è¡Œè®¡ç®—ï¼Œæ‰¹é‡å¤„ç†æ¯”é€æ¡å¤„ç†æ›´å¿«ã€‚\n",
        "2. **ç»Ÿä¸€æ¥å£**ï¼šæ— è®ºè¾“å…¥å•æ¡è¿˜æ˜¯å¤šæ¡æ•°æ®ï¼Œæ¨¡å‹å§‹ç»ˆä»¥æ‰¹æ¬¡å½¢å¼å¤„ç†ï¼ˆå•æ¡æ—¶ `batch_size=1`ï¼‰ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### è¡¥å……ï¼šæ‰¹å¤„ç†å¤šåºåˆ—\n",
        "å¦‚æœè¾“å…¥å¤šä¸ªåºåˆ—ï¼Œéœ€ä¿è¯é•¿åº¦ä¸€è‡´ï¼ˆé€šè¿‡å¡«å……ï¼‰ï¼š\n",
        "```python\n",
        "batched_ids = [\n",
        "    [1, 2, 3],\n",
        "    [1, 2, 0]  # å‡è®¾ç”¨0å¡«å……åˆ°é•¿åº¦3\n",
        "]\n",
        "input_ids = torch.tensor(batched_ids)  # å½¢çŠ¶ (2, 3)\n",
        "model(input_ids)\n",
        "```\n",
        "\n",
        "æ€»ç»“ï¼š**å§‹ç»ˆç¡®ä¿è¾“å…¥å¼ é‡æœ‰ `batch_size` ç»´åº¦**ï¼Œè¿™æ˜¯æˆåŠŸè¿è¡Œçš„å…³é”®ï¼"
      ],
      "metadata": {
        "id": "4O9q-mKk3D3e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dh5Z3hyX1oef",
        "outputId": "57e2e6a2-c916-419e-eed3-a603e40a42b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2b9fc1b6cd40>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# This line will fail.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    975\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m         distilbert_output = self.distilbert(\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/distilbert/modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   5165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5166\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5167\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5168\u001b[0m             warn_string = (\n\u001b[1;32m   5169\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0YDI5uA61oeg",
        "outputId": "fc651245-46c1-4b1c-9534-ee243c6f5d94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n"
          ]
        }
      ],
      "source": [
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(tokenized_inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1aG75ZxX1oeg",
        "outputId": "0a17c5ac-d954-41c4-fbb9-f810e2394bb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "#æŠŠidsè½¬åŒ–ä¸ºtensorï¼Œå†è¾“å…¥è¿›è¡Œæ¨¡å‹ä¸­\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å¡«å……è¾“å…¥ï¼ˆPadding the Inputsï¼‰\n",
        "\n",
        "ä»¥ä¸‹åˆ—è¡¨çš„åˆ—è¡¨æ— æ³•ç›´æ¥è½¬æ¢ä¸ºå¼ é‡ï¼š\n",
        "\n",
        "```python\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]\n",
        "```\n",
        "\n",
        "ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ **å¡«å……ï¼ˆpaddingï¼‰** æ¥ä½¿å¼ é‡æˆä¸ºçŸ©å½¢å½¢çŠ¶ã€‚å¡«å……é€šè¿‡å‘è¾ƒçŸ­çš„å¥å­æ·»åŠ ä¸€ä¸ªç‰¹æ®Šçš„ **å¡«å……æ ‡è®°ï¼ˆpadding tokenï¼‰**ï¼Œç¡®ä¿æ‰€æœ‰å¥å­çš„é•¿åº¦ç›¸åŒã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ 10 ä¸ªå¥å­ï¼Œæ¯ä¸ªå¥å­æœ‰ 10 ä¸ªå•è¯ï¼Œè€Œ 1 ä¸ªå¥å­æœ‰ 20 ä¸ªå•è¯ï¼Œå¡«å……ä¼šç¡®ä¿æ‰€æœ‰å¥å­éƒ½å˜æˆ 20 ä¸ªå•è¯ã€‚åœ¨æˆ‘ä»¬çš„ä¾‹å­ä¸­ï¼Œå¡«å……åçš„å¼ é‡å¦‚ä¸‹ï¼š\n",
        "\n",
        "```python\n",
        "padding_id = 100  # å‡è®¾å¡«å……æ ‡è®°çš„IDæ˜¯100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],          # ç¬¬ä¸€ä¸ªå¥å­ï¼ˆé•¿åº¦3ï¼‰\n",
        "    [200, 200, padding_id],   # ç¬¬äºŒä¸ªå¥å­ï¼ˆå¡«å……åé•¿åº¦3ï¼‰\n",
        "]\n",
        "```\n",
        "\n",
        "å¡«å……æ ‡è®°çš„ ID å¯ä»¥é€šè¿‡ `tokenizer.pad_token_id` è·å–ã€‚è®©æˆ‘ä»¬ä½¿ç”¨å®ƒï¼Œå¹¶åˆ†åˆ«å°†ä¸¤ä¸ªå¥å­å•ç‹¬è¾“å…¥æ¨¡å‹ï¼Œä»¥åŠç»„æˆæ‰¹æ¬¡åè¾“å…¥æ¨¡å‹ï¼š\n",
        "\n",
        "```python\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]  # å¥å­1ï¼ˆé•¿åº¦3ï¼‰\n",
        "sequence2_ids = [[200, 200]]       # å¥å­2ï¼ˆé•¿åº¦2ï¼‰\n",
        "batched_ids = [\n",
        "    [200, 200, 200],               # å¥å­1\n",
        "    [200, 200, tokenizer.pad_token_id],  # å¥å­2ï¼ˆå¡«å……åé•¿åº¦3ï¼‰\n",
        "]\n",
        "\n",
        "# åˆ†åˆ«è¾“å…¥æ¨¡å‹\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "\n",
        "# æ‰¹é‡è¾“å…¥æ¨¡å‹\n",
        "print(model(torch.tensor(batched_ids)).logits)\n",
        "```\n",
        "\n",
        "è¾“å‡ºç»“æœï¼š\n",
        "\n",
        "```\n",
        "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)       # å¥å­1å•ç‹¬è¾“å…¥\n",
        "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)       # å¥å­2å•ç‹¬è¾“å…¥\n",
        "tensor([[ 1.5694, -1.3895],                                 # æ‰¹é‡è¾“å…¥\n",
        "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n",
        "```\n",
        "\n",
        "### é—®é¢˜ï¼šæ‰¹é‡é¢„æµ‹çš„ logits ä¸æ­£ç¡®\n",
        "åœ¨æ‰¹é‡é¢„æµ‹çš„ç»“æœä¸­ï¼Œç¬¬äºŒè¡Œçš„ logits åº”è¯¥å’Œå•ç‹¬è¾“å…¥å¥å­2æ—¶çš„ logitsï¼ˆ`[0.5803, -0.4125]`ï¼‰ç›¸åŒï¼Œä½†æˆ‘ä»¬å´å¾—åˆ°äº†å®Œå…¨ä¸åŒçš„å€¼ï¼ˆ`[1.3373, -1.2163]`ï¼‰ï¼\n",
        "\n",
        "### åŸå› ï¼šæ³¨æ„åŠ›æœºåˆ¶ä¼šè€ƒè™‘å¡«å……æ ‡è®°\n",
        "Transformer æ¨¡å‹çš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯ **æ³¨æ„åŠ›å±‚ï¼ˆattention layersï¼‰**ï¼Œå®ƒä»¬ä¼šå¯¹åºåˆ—ä¸­çš„æ¯ä¸ª token è¿›è¡Œä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚è¿™æ„å‘³ç€ï¼Œå¡«å……æ ‡è®°ï¼ˆå¦‚ `padding_id=100`ï¼‰ä¹Ÿä¼šè¢«çº³å…¥è®¡ç®—ï¼Œå› ä¸ºæ³¨æ„åŠ›æœºåˆ¶ä¼šå…³æ³¨åºåˆ—ä¸­çš„æ‰€æœ‰ tokenï¼ˆåŒ…æ‹¬å¡«å…… tokenï¼‰ã€‚\n",
        "\n",
        "### è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰\n",
        "ä¸ºäº†ç¡®ä¿ï¼š\n",
        "1. **å•ç‹¬è¾“å…¥ä¸åŒé•¿åº¦çš„å¥å­** å’Œ  \n",
        "2. **æ‰¹é‡è¾“å…¥å¡«å……åçš„å¥å­**  \n",
        "\n",
        "å¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œæˆ‘ä»¬éœ€è¦å‘Šè¯‰æ³¨æ„åŠ›å±‚ **å¿½ç•¥å¡«å……æ ‡è®°**ã€‚è¿™å¯ä»¥é€šè¿‡ **æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰** å®ç°ã€‚\n",
        "\n",
        "#### æ³¨æ„åŠ›æ©ç çš„ä½œç”¨\n",
        "- **1**ï¼šè¡¨ç¤ºæ¨¡å‹åº”å…³æ³¨è¯¥ tokenï¼ˆçœŸå® tokenï¼‰ã€‚  \n",
        "- **0**ï¼šè¡¨ç¤ºæ¨¡å‹åº”å¿½ç•¥è¯¥ tokenï¼ˆå¡«å…… tokenï¼‰ã€‚  \n",
        "\n",
        "#### ä¿®æ”¹åçš„ä»£ç ï¼ˆä½¿ç”¨ `tokenizer` è‡ªåŠ¨ç”Ÿæˆæ©ç ï¼‰\n",
        "```python\n",
        "# ä½¿ç”¨ tokenizer è‡ªåŠ¨å¤„ç†å¡«å……å’Œæ©ç \n",
        "batched_inputs = tokenizer(\n",
        "    [\"Sentence one\", \"Sentence two\"],  # ä¸¤ä¸ªå¥å­\n",
        "    padding=True,                      # è‡ªåŠ¨å¡«å……\n",
        "    return_tensors=\"pt\",               # è¿”å› PyTorch å¼ é‡\n",
        ")\n",
        "\n",
        "# è¾“å…¥æ¨¡å‹ï¼ˆåŒ…å« input_ids å’Œ attention_maskï¼‰\n",
        "outputs = model(**batched_inputs)\n",
        "print(outputs.logits)\n",
        "```\n",
        "\n",
        "è¿™æ ·ï¼Œæ¨¡å‹ä¼šæ­£ç¡®å¿½ç•¥å¡«å……éƒ¨åˆ†ï¼Œæ‰¹é‡è®¡ç®—çš„ç»“æœå°†ä¸å•ç‹¬è®¡ç®—çš„ç»“æœä¸€è‡´ã€‚\n",
        "\n",
        "---\n",
        "\n",
        "### å…³é”®æ€»ç»“\n",
        "1. **å¡«å……ï¼ˆPaddingï¼‰**ï¼šä½¿æ‰€æœ‰å¥å­é•¿åº¦ç›¸åŒï¼Œä»¥ä¾¿ç»„æˆå¼ é‡ã€‚  \n",
        "2. **æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰**ï¼šç¡®ä¿æ¨¡å‹å¿½ç•¥å¡«å…… tokenï¼Œé¿å…å½±å“é¢„æµ‹ç»“æœã€‚  \n",
        "3. **æœ€ä½³å®è·µ**ï¼šç›´æ¥ä½¿ç”¨ `tokenizer(..., padding=True, return_tensors=\"pt\")`ï¼Œè®©åˆ†è¯å™¨è‡ªåŠ¨å¤„ç†å¡«å……å’Œæ©ç ï¼Œè€Œéæ‰‹åŠ¨æ“ä½œã€‚  \n",
        "\n",
        "> ğŸ“Œ **è®°ä½**ï¼š**å¡«å…… + æ©ç ** æ˜¯æ­£ç¡®å¤„ç†å˜é•¿åºåˆ—æ‰¹å¤„ç†çš„å…³é”®ï¼"
      ],
      "metadata": {
        "id": "tiAkAWOY4V5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ldP7HMUq1oeh"
      },
      "outputs": [],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fDzeNdkz1oeh"
      },
      "outputs": [],
      "source": [
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3UNff1_T1oei",
        "outputId": "a0c95c5c-da4e-40b0-abfd-1abfb3dd9248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "NncIW4s_1oei",
        "outputId": "7651c384-a4cc-4804-e15f-b26771a4109b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.5694, -1.3895],\n",
            "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ç»ƒä¹ 1\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# å¥å­\n",
        "sequence1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "sequence2 = \"I hate this so much!\"\n",
        "\n",
        "# å•ç‹¬åˆ†è¯å¹¶è½¬æ¢ä¸ºID\n",
        "tokens1 = tokenizer.tokenize(sequence1)\n",
        "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "\n",
        "tokens2 = tokenizer.tokenize(sequence2)\n",
        "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "\n",
        "print(\"å¥å­1çš„ID:\", ids1)\n",
        "print(\"å¥å­2çš„ID:\", ids2)"
      ],
      "metadata": {
        "id": "KBF3ixkg4p_m",
        "outputId": "90df970d-e43e-4d6f-8e5a-3826092ddcb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å¥å­1çš„ID: [1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
            "å¥å­2çš„ID: [1045, 5223, 2023, 2061, 2172, 999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è½¬æ¢ä¸ºå¼ é‡ï¼ˆæ·»åŠ æ‰¹æ¬¡ç»´åº¦ï¼‰\n",
        "input_ids1 = torch.tensor([ids1])\n",
        "input_ids2 = torch.tensor([ids2])\n",
        "\n",
        "# å•ç‹¬é¢„æµ‹\n",
        "output1 = model(input_ids1)\n",
        "output2 = model(input_ids2)\n",
        "\n",
        "print(\"å¥å­1çš„logits:\", output1.logits)\n",
        "print(\"å¥å­2çš„logits:\", output2.logits)"
      ],
      "metadata": {
        "id": "6rNc77vr4tHV",
        "outputId": "2b536b1a-7ed3-4349-c434-28f8d9a1b17a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å¥å­1çš„logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n",
            "å¥å­2çš„logits: tensor([[ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è®¡ç®—æœ€å¤§é•¿åº¦\n",
        "max_length = max(len(ids1), len(ids2))\n",
        "\n",
        "# å¯¹å¥å­2å¡«å……ï¼ˆå¥å­1å·²ç»æ˜¯æœ€é•¿ï¼Œæ— éœ€å¡«å……ï¼‰\n",
        "padded_ids2 = ids2 + [tokenizer.pad_token_id] * (max_length - len(ids2))\n",
        "\n",
        "# å¡«å……åçš„æ‰¹æ¬¡è¾“å…¥\n",
        "batched_ids = [\n",
        "    ids1,              # å¥å­1ï¼ˆé•¿åº¦14ï¼‰\n",
        "    padded_ids2        # å¥å­2ï¼ˆå¡«å……åé•¿åº¦14ï¼‰\n",
        "]\n",
        "\n",
        "# åˆ›å»ºæ³¨æ„åŠ›æ©ç ï¼ˆ1=çœŸå®tokenï¼Œ0=å¡«å……tokenï¼‰\n",
        "attention_mask = [\n",
        "    [1] * len(ids1) + [0] * (max_length - len(ids1)),  # å¥å­1ï¼ˆæ— éœ€å¡«å……ï¼Œå…¨1ï¼‰\n",
        "    [1] * len(ids2) + [0] * (max_length - len(ids2))   # å¥å­2ï¼ˆå‰5ä¸ªæ˜¯çœŸå®tokenï¼‰\n",
        "]\n",
        "\n",
        "# è½¬æ¢ä¸ºå¼ é‡\n",
        "input_ids = torch.tensor(batched_ids)\n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "print(\"å¡«å……åçš„input_ids:\\n\", input_ids)\n",
        "print(\"æ³¨æ„åŠ›æ©ç :\\n\", attention_mask)"
      ],
      "metadata": {
        "id": "71GlRqpZ4v8D",
        "outputId": "9344fbbf-d810-44ff-d12d-ca1f2a72567b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "å¡«å……åçš„input_ids:\n",
            " tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012],\n",
            "        [ 1045,  5223,  2023,  2061,  2172,   999,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "æ³¨æ„åŠ›æ©ç :\n",
            " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# è¾“å…¥æ¨¡å‹ï¼ˆåŒæ—¶ä¼ é€’input_idså’Œattention_maskï¼‰\n",
        "batched_output = model(input_ids, attention_mask=attention_mask)\n",
        "print(\"æ‰¹é‡é¢„æµ‹çš„logits:\\n\", batched_output.logits)"
      ],
      "metadata": {
        "id": "mgphqm6B5Lek",
        "outputId": "33cd9fa1-be25-4c79-fc29-319b3f7bbcac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "æ‰¹é‡é¢„æµ‹çš„logits:\n",
            " tensor([[-2.7276,  2.8789],\n",
            "        [ 3.1931, -2.6685]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# é”™è¯¯ç¤ºèŒƒï¼šä¸å¸¦æ©ç çš„æ‰¹é‡è¾“å…¥\n",
        "wrong_output = model(input_ids)  # ä¸ä¼ é€’attention_mask\n",
        "print(\"ä¸å¸¦æ©ç çš„logitsï¼ˆé”™è¯¯ï¼‰:\\n\", wrong_output.logits)"
      ],
      "metadata": {
        "id": "kzSxpv6S5Ms9",
        "outputId": "1c47f187-91dd-4b96-c13d-4e80e3b37703",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ä¸å¸¦æ©ç çš„logitsï¼ˆé”™è¯¯ï¼‰:\n",
            " tensor([[-2.7276,  2.8789],\n",
            "        [ 2.5423, -2.1265]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### å¤„ç†é•¿åºåˆ—é—®é¢˜ï¼ˆLonger Sequencesï¼‰\n",
        "\n",
        "Transformer æ¨¡å‹å¯¹è¾“å…¥åºåˆ—çš„é•¿åº¦æœ‰é™åˆ¶ï¼ˆé€šå¸¸ä¸º 512 æˆ– 1024 ä¸ª tokenï¼‰ã€‚å¦‚æœåºåˆ—è¶…å‡ºé™åˆ¶ï¼Œæ¨¡å‹ä¼šæŠ¥é”™ã€‚ä»¥ä¸‹æ˜¯è§£å†³æ–¹æ¡ˆå’Œç¤ºä¾‹ï¼š\n",
        "\n",
        "---\n",
        "\n",
        "#### âŒ é—®é¢˜ç¤ºä¾‹\n",
        "å‡è®¾æœ‰ä¸€ä¸ªè¶…é•¿åºåˆ—ï¼š\n",
        "```python\n",
        "long_sequence = \"This is a very long sequence... [hundreds of words later] ... and it will crash the model.\"\n",
        "```\n",
        "\n",
        "#### âœ… è§£å†³æ–¹æ¡ˆ 1ï¼šæˆªæ–­åºåˆ—ï¼ˆTruncationï¼‰\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# è‡ªåŠ¨æˆªæ–­åˆ°æ¨¡å‹æœ€å¤§é•¿åº¦ï¼ˆå¦‚512ï¼‰\n",
        "truncated_sequence = tokenizer(\n",
        "    long_sequence,\n",
        "    truncation=True,  # å¯ç”¨æˆªæ–­\n",
        "    max_length=512,   # æ˜ç¡®æŒ‡å®šæœ€å¤§é•¿åº¦ï¼ˆå¯é€‰ï¼‰\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "print(\"æˆªæ–­åçš„è¾“å…¥é•¿åº¦:\", len(truncated_sequence[\"input_ids\"][0]))\n",
        "```\n",
        "\n",
        "#### âœ… è§£å†³æ–¹æ¡ˆ 2ï¼šä½¿ç”¨æ”¯æŒé•¿åºåˆ—çš„æ¨¡å‹\n",
        "```python\n",
        "from transformers import LongformerModel\n",
        "\n",
        "# åŠ è½½é•¿åºåˆ—ä¸“ç”¨æ¨¡å‹ï¼ˆå¦‚Longformerï¼Œæ”¯æŒ4096 tokenï¼‰\n",
        "model = LongformerModel.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### å…³é”®ç»†èŠ‚\n",
        "1. **é»˜è®¤é™åˆ¶**ï¼š\n",
        "   - BERTç±»æ¨¡å‹ï¼šé€šå¸¸ 512 token\n",
        "   - GPTç±»æ¨¡å‹ï¼šé€šå¸¸ 1024 token\n",
        "\n",
        "2. **æˆªæ–­æ–¹å¼**ï¼š\n",
        "   ```python\n",
        "   # ä¿ç•™å¼€å¤´éƒ¨åˆ†ï¼ˆé»˜è®¤ï¼‰\n",
        "   tokenizer(sequence, truncation=True, max_length=100)\n",
        "\n",
        "   # ä¿ç•™ç»“å°¾éƒ¨åˆ†\n",
        "   tokenizer(sequence, truncation=\"only_last\", max_length=100)\n",
        "   ```\n",
        "\n",
        "3. **é•¿åºåˆ—æ¨¡å‹ç¤ºä¾‹**ï¼š\n",
        "   - **Longformer**ï¼šæ”¯æŒ 4096 tokenï¼ˆé€‚åˆæ–‡æ¡£å¤„ç†ï¼‰\n",
        "   - **LED**ï¼šæ”¯æŒ 16384 tokenï¼ˆé€‚åˆè¶…é•¿æ–‡æœ¬æ‘˜è¦ï¼‰\n",
        "\n",
        "---\n",
        "\n",
        "### å®é™…åº”ç”¨å»ºè®®\n",
        "```python\n",
        "# æœ€ä½³å®è·µï¼šåŒæ—¶å¤„ç†å¡«å……å’Œæˆªæ–­\n",
        "inputs = tokenizer(\n",
        "    [sequence1, sequence2],\n",
        "    padding=True,      # è‡ªåŠ¨å¡«å……\n",
        "    truncation=True,   # è‡ªåŠ¨æˆªæ–­\n",
        "    max_length=512,    # è®¾ç½®æœ€å¤§å€¼\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "outputs = model(**inputs)\n",
        "```\n",
        "\n",
        "> ğŸ“Œ **æ³¨æ„**ï¼šå¦‚æœä»»åŠ¡å¿…é¡»ä½¿ç”¨å®Œæ•´é•¿åºåˆ—ï¼ˆå¦‚æ³•å¾‹æ–‡æ¡£åˆ†æï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©ä¸“ç”¨æ¨¡å‹è€Œéæˆªæ–­ã€‚"
      ],
      "metadata": {
        "id": "DhNeK6zP5kXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xFUyCHkG1oej",
        "outputId": "5e3c8977-eee9-4363-ece6-3fe75044896d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'max_sequence_length' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-af6acd4b2a78>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'max_sequence_length' is not defined"
          ]
        }
      ],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "è¿™ä¸ªé”™è¯¯æ˜¯å› ä¸ºä½ ç›´æ¥ä½¿ç”¨äº†æœªå®šä¹‰çš„å˜é‡ `max_sequence_length`ã€‚Pythonä¼šæŠ¥`NameError`ï¼Œè¡¨ç¤ºå®ƒä¸çŸ¥é“`max_sequence_length`æ˜¯ä»€ä¹ˆã€‚\n",
        "\n",
        "### é”™è¯¯åŸå› åˆ†æ\n",
        "```python\n",
        "sequence = sequence[:max_sequence_length]  # æŠ¥é”™\n",
        "```\n",
        "- ä½ è¯•å›¾ç”¨åˆ‡ç‰‡æ“ä½œæˆªå–åºåˆ—ï¼Œä½†`max_sequence_length`è¿™ä¸ªå˜é‡ä»æœªè¢«å®šä¹‰è¿‡\n",
        "- Pythonä¸çŸ¥é“`max_sequence_length`åº”è¯¥å–å€¼å¤šå°‘\n",
        "\n",
        "### æ­£ç¡®åšæ³•\n",
        "\n",
        "#### æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨å›ºå®šå€¼\n",
        "```python\n",
        "max_sequence_length = 512  # å…ˆå®šä¹‰è¿™ä¸ªå˜é‡\n",
        "sequence = sequence[:max_sequence_length]  # ç°åœ¨å¯ä»¥æ­£å¸¸å·¥ä½œ\n",
        "```\n",
        "\n",
        "#### æ–¹æ³•2ï¼šä½¿ç”¨tokenizerçš„è‡ªåŠ¨æˆªæ–­ï¼ˆæ¨èï¼‰\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = tokenizer(sequence, truncation=True, max_length=512)  # è‡ªåŠ¨å¤„ç†æˆªæ–­\n",
        "```\n",
        "\n",
        "#### æ–¹æ³•3ï¼šè·å–æ¨¡å‹çš„æœ€å¤§é•¿åº¦\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "max_sequence_length = tokenizer.model_max_length  # è·å–æ¨¡å‹æ”¯æŒçš„æœ€å¤§é•¿åº¦\n",
        "sequence = sequence[:max_sequence_length]\n",
        "```\n",
        "\n",
        "### å…³é”®åŒºåˆ«\n",
        "| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n",
        "|------|------|------|\n",
        "| å›ºå®šå€¼ | ç®€å•ç›´æ¥ | éœ€è¦çŸ¥é“æ¨¡å‹é™åˆ¶ |\n",
        "| tokenizerè‡ªåŠ¨æˆªæ–­ | æ™ºèƒ½å¤„ç†ï¼Œä¿ç•™é‡è¦éƒ¨åˆ† | éœ€è¦è°ƒç”¨tokenizer |\n",
        "| è·å–æ¨¡å‹æœ€å¤§é•¿åº¦ | åŠ¨æ€é€‚åº”ä¸åŒæ¨¡å‹ | ä»éœ€æ‰‹åŠ¨åˆ‡ç‰‡ |\n",
        "\n",
        "### æœ€ä½³å®è·µå»ºè®®\n",
        "```python\n",
        "# æ¨èåšæ³•ï¼šä½¿ç”¨tokenizerè‡ªåŠ¨å¤„ç†\n",
        "inputs = tokenizer(\n",
        "    sequence,\n",
        "    truncation=True,  # è‡ªåŠ¨æˆªæ–­\n",
        "    max_length=512,   # æ˜ç¡®é™åˆ¶é•¿åº¦\n",
        "    return_tensors=\"pt\"  # è¿”å›PyTorchå¼ é‡\n",
        ")\n",
        "```\n",
        "\n",
        "> è¿™æ ·æ—¢é¿å…äº†æœªå®šä¹‰å˜é‡çš„é”™è¯¯ï¼Œåˆèƒ½ç¡®ä¿æ­£ç¡®å¤„ç†åºåˆ—é•¿åº¦é™åˆ¶ã€‚Tokenizerä¼šè‡ªåŠ¨å¤„ç†æˆªæ–­ä½ç½®ï¼Œé€šå¸¸æ¯”ç®€å•çš„å‰512å­—ç¬¦æˆªå–æ•ˆæœæ›´å¥½ã€‚"
      ],
      "metadata": {
        "id": "sOzAehYf58Lc"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}