{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VivianOuou/NLP-Course/blob/main/course/en/chapter2/section4_Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mpM_ZAxWVI0"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkMtUsADWVI2"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5FUejGXlWVI2",
        "outputId": "22f56c6e-3bc2-45ef-828f-45e655667bee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (5.29.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **基于单词的分词（Word-based Tokenization）方法总结**\n",
        "\n",
        "#### **方法概述**\n",
        "基于单词的分词（Word-based Tokenization）是最直观的分词方式，通常按照**空格和标点符号**将文本拆分成单词，并为每个单词分配唯一的ID。例如：\n",
        "```python\n",
        "text = \"Jim Henson was a puppeteer\"\n",
        "tokens = text.split()  # ['Jim', 'Henson', 'was', 'a', 'puppeteer']\n",
        "```\n",
        "\n",
        "#### **优点**\n",
        "✅ **简单易用**  \n",
        "- 实现容易，仅需按空格或标点分割文本，适合基础NLP任务。  \n",
        "- 人类可读性强，分词结果直接对应自然语言单词。\n",
        "\n",
        "✅ **语义粒度较细**  \n",
        "- 每个单词独立编码，适合处理固定词汇表的任务（如分类、检索）。  \n",
        "\n",
        "✅ **适用于小规模、领域特定的数据**  \n",
        "- 如果词汇表可控（如医学、法律术语），单词级分词可能足够高效。\n",
        "\n",
        "---\n",
        "\n",
        "#### **缺点**\n",
        "❌ **词汇表膨胀（Vocabulary Bloat）**  \n",
        "- 英语约有50万+单词，若考虑变形（如\"run\"→\"running\"），词汇量会极大。  \n",
        "- 存储和计算成本高，模型输入层需要极大矩阵（如 `500,000×768` 的嵌入矩阵）。\n",
        "\n",
        "❌ **无法处理未登录词（OOV, Out-of-Vocabulary）**  \n",
        "- 遇到不在词汇表的单词（如专业术语、拼写错误）会被替换为 `[UNK]`，导致信息丢失。  \n",
        "- 例如：\n",
        "  ```python\n",
        "  vocab = {\"cat\": 1, \"dog\": 2, \"[UNK]\": 0}\n",
        "  encode(\"puppy\") → [0]  # 完全丢失语义\n",
        "  ```\n",
        "\n",
        "❌ **无法捕捉词形变化（Morphology）**  \n",
        "- 将不同形式的单词视为完全无关：  \n",
        "  - \"dog\" 和 \"dogs\" → 不同ID  \n",
        "  - \"run\" 和 \"ran\" → 无关联  \n",
        "\n",
        "❌ **对空格和标点敏感**  \n",
        "- 某些语言（如中文、日语）没有明确空格分隔，难以直接应用。  \n",
        "- 缩写和连字符可能被错误分割（如 \"can't\" → `[\"can\", \"'\", \"t\"]`）。\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**建议**：除非处理特定领域数据，否则优先选择基于子词的分词方法（如Hugging Face的`BertTokenizer`）。"
      ],
      "metadata": {
        "id": "PGdiXS7AYYAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **基于字符的分词（Character-based Tokenization）方法总结**\n",
        "\n",
        "#### **方法概述**\n",
        "基于字符的分词（Character-based Tokenization）将文本拆分为**单个字符**（字母、标点、空格），而不是单词。例如：\n",
        "```python\n",
        "text = \"dog\"\n",
        "tokens = list(text)  # ['d', 'o', 'g']\n",
        "```\n",
        "\n",
        "#### **优点**\n",
        "✅ **极小的词汇量**  \n",
        "- 仅需覆盖语言的基本字符集（如ASCII共256个字符，中文约5000+常用字）。  \n",
        "- 词汇量通常 **< 1,000**，远小于单词级分词（50万+）。\n",
        "\n",
        "✅ **几乎无OOV（未登录词）问题**  \n",
        "- 任何单词均可由字符组合而成，避免 `[UNK]` 问题。  \n",
        "  ```python\n",
        "  # 即使词汇表没有\"puppy\"，仍可拆解为：\n",
        "  list(\"puppy\") → ['p', 'u', 'p', 'p', 'y']\n",
        "  ```\n",
        "\n",
        "✅ **跨语言兼容性强**  \n",
        "- 适合无空格语言（如中文、日语）：  \n",
        "  ```python\n",
        "  list(\"深度学习\") → ['深', '度', '学', '习']\n",
        "  ```\n",
        "- 对拼写错误、缩写更鲁棒（如 \"can't\" → `['c', 'a', 'n', \"'\", 't']`）。\n",
        "\n",
        "---\n",
        "\n",
        "#### **缺点**\n",
        "❌ **序列长度爆炸**  \n",
        "- 一个单词变为多个字符，输入序列大幅变长：  \n",
        "  ```python\n",
        "  \"Transformers\" → 12个字符（原本1个单词）\n",
        "  ```\n",
        "  - 导致计算量增加（Transformer的复杂度与序列长度平方相关）。\n",
        "\n",
        "❌ **语义粒度太细**  \n",
        "- 单个字符无明确语义（如 'd' 与 'o' 组合才形成 \"do\" 的含义）。  \n",
        "- 模型需额外学习字符组合的语义，训练难度更高。\n",
        "\n",
        "❌ **对空格和标点敏感**  \n",
        "- 空格可能被当作普通字符处理，需额外规则：  \n",
        "  ```python\n",
        "  \"hello world\" → ['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd']\n",
        "  ```\n",
        "\n",
        "❌ **信息密度低**  \n",
        "- 拉丁语系中，单个字符信息量少（中文/日文稍好）：  \n",
        "  - 英文：'a' 单独无意义  \n",
        "  - 中文：'语' 本身有部分语义  \n"
      ],
      "metadata": {
        "id": "5-n7mnZjZzFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **子词分词（Subword Tokenization）方法总结**\n",
        "\n",
        "#### **核心思想**\n",
        "子词分词（Subword Tokenization）是一种**平衡词汇量与语义表达**的分词方法，其核心原则是：\n",
        "- **高频词保留完整**（如 \"the\", \"cat\"）  \n",
        "- **低频词拆分为有意义的子词**（如 \"annoyingly\" → `\"annoying\" + \"ly\"`）  \n",
        "\n",
        "#### **主流算法对比**\n",
        "| **算法**         | **代表模型** | **特点**                                                                 | **示例**                          |\n",
        "|------------------|--------------|--------------------------------------------------------------------------|-----------------------------------|\n",
        "| **Byte-Pair Encoding (BPE)** | GPT-2, GPT-3 | 通过迭代合并最高频的字符对构建词汇表                                    | \"tokenization\" → `[\"token\", \"ization\"]` |\n",
        "| **WordPiece**    | BERT         | 类似BPE，但基于概率合并（最大化语言模型似然）                           | \"unhappiness\" → `[\"un\", \"##happiness\"]` |\n",
        "| **Unigram LM**   | XLNet, ALBERT| 从大词汇表开始，逐步删除低概率子词                                      | \"深度学习\" → `[\"深\", \"度\", \"学习\"]`     |\n",
        "| **SentencePiece**| T5, mBERT    | 直接处理原始文本（无需预分词），支持多语言                              | \"Hello world!\" → `[\"▁He\", \"llo\", \"▁world\", \"!\"]` |\n",
        "\n",
        "---\n",
        "\n",
        "### **三大优势**\n",
        "1. **词汇量可控**  \n",
        "   - 典型词汇量：30,000–50,000（远小于单词级分词的50万+）  \n",
        "   - 例如：BERT的`bert-base-uncased`词汇量=30,522  \n",
        "\n",
        "2. **近乎零OOV问题**  \n",
        "   - 任何单词均可拆解为子词：  \n",
        "     ```python\n",
        "     # 即使词汇表没有\"tokenization\"\n",
        "     \"tokenization\" → \"token\" + \"ization\"  # 两个已知子词\n",
        "     ```\n",
        "\n",
        "3. **保留语义组合性**  \n",
        "   - 子词本身携带语义：  \n",
        "     - `\"##ly\"` 表示副词（如 \"quickly\" → `\"quick\" + \"##ly\"`）  \n",
        "     - `\"##ization\"` 表示名词化（如 \"modernization\" → `\"modern\" + \"##ization\"`）  \n",
        "\n",
        "---\n",
        "\n",
        "### **实际应用示例**\n",
        "#### **1. 使用Hugging Face Tokenizer**\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 加载BERT的WordPiece分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokens = tokenizer.tokenize(\"unhappiness\")\n",
        "print(tokens)  # ['un', '##ha', '##pp', '##iness']\n",
        "```\n",
        "\n",
        "#### **2. 处理多语言（SentencePiece）**\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 加载T5的SentencePiece分词器\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\n",
        "tokens = tokenizer.tokenize(\"こんにちは世界！\")  # 日语\"你好世界！\"\n",
        "print(tokens)  # ['▁', 'こん', 'にち', 'は', '世界', '!']\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **为何优于字符/单词级分词？**\n",
        "| **场景**               | **单词级**               | **字符级**               | **子词级**              |\n",
        "|------------------------|--------------------------|--------------------------|-------------------------|\n",
        "| 词汇量                 | 极大（50万+）            | 极小（<1,000）           | 中等（3万–5万）         |\n",
        "| OOV问题                | 严重                     | 无                       | 几乎无                  |\n",
        "| 语义粒度               | 过粗（无法处理词形变化） | 过细（字符无独立语义）   | 适中（保留语义单元）    |\n",
        "| 序列长度               | 短                       | 极长                     | 中等                    |\n",
        "| 适合任务               | 固定词汇表任务           | 语音/拼写错误处理        | 通用NLP任务             |\n",
        "\n",
        "---\n",
        "\n",
        "### **语言适应性**\n",
        "1. **黏着语（Agglutinative Languages）**  \n",
        "   - 如土耳其语、芬兰语：  \n",
        "     ```python\n",
        "     # 土耳其语\"öğretmenlerimizden\"（来自我们的老师们）\n",
        "     → [\"öğret\", \"##men\", \"##ler\", \"##imiz\", \"##den\"]\n",
        "     ```\n",
        "2. **屈折语（Fusional Languages）**  \n",
        "   - 如英语、德语：  \n",
        "     ```python\n",
        "     \"running\" → [\"run\", \"##n\", \"##ing\"]\n",
        "     ```\n",
        "3. **孤立语（Isolating Languages）**  \n",
        "   - 如汉语：  \n",
        "     ```python\n",
        "     \"深度学习\" → [\"深\", \"度\", \"学习\"]\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **进阶话题**\n",
        "1. **混合分词策略**  \n",
        "   - 对常见词保留完整，罕见词回退到字符级：\n",
        "     ```python\n",
        "     \"ChatGPT\" → [\"Chat\", \"G\", \"PT\"]  # GPT-4的处理方式\n",
        "     ```\n",
        "2. **跨语言迁移**  \n",
        "   - 使用SentencePiece训练统一的多语言分词器（如mBERT支持100+语言）。\n",
        "\n",
        "3. **领域自适应**  \n",
        "   - 在专业领域（生物、法律）上继续训练分词器：\n",
        "     ```python\n",
        "     # 添加生物医学术语到词汇表\n",
        "     tokenizer.add_tokens([\"EGFR\", \"CRISPR-Cas9\"])\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### **总结**\n",
        "- **推荐使用子词分词**：在绝大多数NLP任务中（如文本分类、翻译、生成），子词分词提供了最佳平衡。  \n",
        "- **工具推荐**：Hugging Face的`transformers`库支持所有主流子词算法（WordPiece/BPE/SentencePiece）。  \n",
        "- **实践建议**：直接使用预训练模型的分词器，而非从头训练。  \n",
        "\n",
        "通过子词分词，我们能够以**紧凑的词汇表**覆盖**近乎无限的单词表达**，这正是现代Transformer模型高效处理多语言、多领域文本的核心基础。"
      ],
      "metadata": {
        "id": "Zsm-h998Z_uZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UNgLTwtlWVI6",
        "outputId": "965dc329-9962-4357-8c9f-1d63e8d0c315",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
          ]
        }
      ],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "分词器（Tokenizer）的加载与保存方法\n",
        "\n",
        "核心方法\n",
        "\n",
        "与加载/保存模型类似，分词器也使用以下两个关键方法：\n",
        "\n",
        "from_pretrained()：从Hugging Face Hub或本地加载分词器\n",
        "save_pretrained()：保存分词器到本地目录"
      ],
      "metadata": {
        "id": "VPy2nBRTbV2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6KVnVVfYWVI7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vLpk5gxfWVI7"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oZ1QnHG8WVI8",
        "outputId": "811c8a3b-4412-4026-8e74-bd5d25ae55fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uVc_hR2UWVI8",
        "outputId": "f1f8f7cc-7c0f-4581-9649-0f920541c4ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('directory_on_my_computer/tokenizer_config.json',\n",
              " 'directory_on_my_computer/special_tokens_map.json',\n",
              " 'directory_on_my_computer/vocab.txt',\n",
              " 'directory_on_my_computer/added_tokens.json',\n",
              " 'directory_on_my_computer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding"
      ],
      "metadata": {
        "id": "cJ6QMoaibZY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LQ4NHjZ7WVI9",
        "outputId": "786db910-9905-46cc-adb8-2fad7321eaef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fcb_x73OWVI-",
        "outputId": "9fc267a1-d052-47e0-96fa-06c8e54fcaf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
          ]
        }
      ],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#作业练习题1\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text1 = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "tokens1 = tokenizer.tokenize(text1)\n",
        "print(tokens1)"
      ],
      "metadata": {
        "id": "HDQzNVPZcEPN",
        "outputId": "6d42b05e-a1d9-44ac-f3e3-0ef183367257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', \"'\", 've', 'been', 'waiting', 'for', 'a', 'hugging', '##face', 'course', 'my', 'whole', 'life', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
        "print(ids1)"
      ],
      "metadata": {
        "id": "pMj7mf9WcNfY",
        "outputId": "56613c49-c9fc-4545-de53-24c0def90fed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#作业练习题2\n",
        "\n",
        "text2 = \"I hate this so much!\"\n",
        "tokens2 = tokenizer.tokenize(text2)\n",
        "print(tokens2)\n",
        "\n",
        "ids2 = tokenizer.convert_tokens_to_ids(tokens2)\n",
        "print(ids2)"
      ],
      "metadata": {
        "id": "pNcxr_17cQ5D",
        "outputId": "d4b32631-db63-49b9-d5f8-91d38793fcb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'hate', 'this', 'so', 'much', '!']\n",
            "[1045, 5223, 2023, 2061, 2172, 999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**加粗文字**### **解码（Decoding）过程详解**\n",
        "\n",
        "#### **1. 核心功能**\n",
        "解码是编码的逆过程，将模型输出的数字ID序列转换回可读文本。通过`decode()`方法实现：\n",
        "```python\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "input_ids = [7993, 170, 11303, 1200, 2443, 1110, 3014]\n",
        "\n",
        "decoded_text = tokenizer.decode(input_ids)\n",
        "print(decoded_text)\n",
        "```\n",
        "**输出**：\n",
        "```\n",
        "\"Using a Transformer network is simple\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. 关键技术细节**\n",
        "| **特性**                | **说明**                                                                 | **示例**                          |\n",
        "|-------------------------|--------------------------------------------------------------------------|-----------------------------------|\n",
        "| **子词重组**            | 自动合并`##`开头的子词                                                  | `[\"Transform\", \"##er\"]` → \"Transformer\" |\n",
        "| **特殊token过滤**       | 默认跳过[CLS]、[SEP]等特殊标记                                          | `[101, 7993, 102]` → \"Using\"     |\n",
        "| **空格恢复**            | 根据语言规则智能添加空格                                                 | `[\"Hello\", \"world\"]` → \"Hello world\" |\n",
        "| **标点处理**            | 自动处理标点粘连问题                                                     | `[\"hello\", \"!\"]]` → \"hello!\"     |\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. 完整工作流程**\n",
        "```mermaid\n",
        "graph LR\n",
        "    A[Input IDs] --> B(Tokenizer.decode)\n",
        "    B --> C[合并子词]\n",
        "    B --> D[过滤特殊token]\n",
        "    B --> E[调整空格]\n",
        "    B --> F[输出可读文本]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. 高级用法示例**\n",
        "**场景1：处理模型生成结果**\n",
        "```python\n",
        "# 假设模型生成输出（带特殊token）\n",
        "generated_ids = [101, 7993, 170, 11303, 1200, 102]  # [CLS] Using a Transformer [SEP]\n",
        "\n",
        "print(tokenizer.decode(generated_ids))  # 输出: \"Using a Transformer\"\n",
        "print(tokenizer.decode(generated_ids, skip_special_tokens=False))  # 输出: \"[CLS] Using a Transformer [SEP]\"\n",
        "```\n",
        "\n",
        "**场景2：控制空格输出**\n",
        "```python\n",
        "# 中文等无空格语言处理\n",
        "chinese_ids = tokenizer.encode(\"深度学习\").input_ids  # [101, 123, 456, 789, 102]\n",
        "print(tokenizer.decode(chinese_ids))  # 输出: \"深度学习\"\n",
        "```\n",
        "\n",
        "**场景3：修复子词断字**\n",
        "```python\n",
        "# 修复子词导致的断字问题\n",
        "broken_ids = [17662, 12172]  # [\"hugging\", \"##face\"]\n",
        "print(tokenizer.decode(broken_ids))  # 输出: \"HuggingFace\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. 为什么解码比简单拼接复杂？**\n",
        "假设直接拼接分词结果：\n",
        "```python\n",
        "tokens = [\"Using\", \"a\", \"Transform\", \"##er\"]\n",
        "bad_text = \" \".join(tokens)  # 得到: \"Using a Transform ##er\"（错误！）\n",
        "```\n",
        "**解码器实际做了**：\n",
        "1. 移除`##`标记 → `\"er\"`\n",
        "2. 合并相邻子词 → `\"Transformer\"`\n",
        "3. 智能空格处理 → `\"a Transformer\"`\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5a2h6LGnbfO8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SWJ9xUjaWVI_",
        "outputId": "2714e119-2f44-4d50-9f04-27a9d2defd8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using a transformer network is simple\n"
          ]
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}